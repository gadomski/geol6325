<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>GEOL 6325 final project</title>

    <link href="bower_components/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="bower_components/bigfoot/dist/bigfoot-default.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <style>
body { position: relative; margin-bottom: 60px; }
    </style>

  </head>
  <body data-spy="scroll" data-target="#navbar-bottom">
    <div class="container">
      <div class="jumbotron" id="top">
        <h3>Calculating LiDAR point cloud uncertainty and propagating uncertainty to snow-water equivalent data products (incomplete)</h3>
        <p>GEOL 6325 final project<br>Peter J. Gadomski</p>
      </div>

      <div class="row">
        <div class="col-xs-10 col-xs-offset-1">
          <p class="lead">
            We implemented a new full-waveform discretization software package for Riegl's .sdf format.
            We attempted and failed to georeference full-waveform airborne LiDAR data.
            We demonstrate the procedure for calculating snow depths and SWE measurements with propagated errors.
          </p>
        </div>
      </div>

      <h1 id="why">Why?</h1>

      <div class="row">
        <div class="col-md-6 col-xs-12">
          <p>
            Snow-water equivalent (SWE) is a measure of the water content in a snowpack.
            SWE measurements over large mountain basins are used by water managers to predict the volume and rate of water production during a melt season.
            SWE measurements also give us a picture of the amount of water stored in the mountains, which helps quantify water shortages or surpluses.
          </p>
        </div>
        <div class="col-md-6 col-xs-12">
          <p>
            It is crucial to provide uncertainty estimates with any measurement, including LiDAR-derived data products.
            However, per-point uncertainties are difficult to calculate and are not commonly used by LiDAR practitioners.
            If uncertainties are provided, they are usually estimated at a dataset-wide level based upon the quality of the geopositioning and the LiDAR manufacturer's quoted specifications.
          </p>
          <p>
            We hope to provide per-point uncertainties derived from the fundamental parameters of the LiDAR georeferencing equation.
          </p>
        </div>
        <div class="col-md-6 col-md-offset-3 col-xs-12">
          <div class="thumbnail">
            <img src="images/swe.jpg" alt="federal">
            <div class="caption">
              <h4>Measuring SWE manually</h4>

              <p>
                Using a federal sampler to measure SWE in the Red Mountain Pass in Colorado.
                Over the course of eight hours of collection, we collected about twenty points over an about five km square area.
              </p>
            </div>
          </div>
        </div>
      </div>
        
      <h1 id="snow-depths-and-densities">Snow depths and densities</h1>

      <div class="row">
        <div class="col-md-6 col-md-offset-3 col-xs-12">
          <div class="thumbnail">
            <img src="images/snow-depth.svg" alt="snow depth">
            <div class="caption">
              <h4>Calculating snow depth from two LiDAR scans</h4>

              <p>
                Subtract the height of the snow-on scan from the point height of the snow-free scan.
                Because points are unordered, a snow-on point will probably not have a snow-free point directly beneath it; use an interpolation method to calculate a snow-free height for each snow-on point.
              </p>
            </div>
          </div>
        </div>
        <div class="col-xs-12">
          <div class="thumbnail">
            <img src="images/snotel.png" alt="snotel">
            <div class="caption">
              <h4>SNOTEL sites in Colorado</h4>

              <p>
              SNOTEL sites transmit weather and snowpack data, including SWE and snow depth.
              SNOTEL locations from <a href="http://www.wcc.nrcs.usda.gov/snow/">NRCS</a>.
              Base data from <a href="http://www.blm.gov/co/st/en/BLM_Programs/geographical_sciences/gis/GeospatialData.html">the BLM</a>.
              </p>
            </div>
          </div>
        </div>
        <div class="col-xs-12">
          <div class="thumbnail">
            <img src="images/beartown.png" alt="snow density">
            <div class="caption">
              <h4>Snow density at one Snotel station</h4>

              <p>
                The Beartown SNOTEL station is near our area of interest.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="row">
        <div class="col-xs-12">
          <p>
          The Natural Resources Convservation Service maintains a network of remote snow monitoring stations called SNOTEL sites.<sup id="fnref:5"><a href="#fn:5">5</a></sup>
          Included in the SNOTEL data products are snow depth and SWE (snow-water equivilant, i.e. the height of water that would be left if you melted a given depth of snow) calculations.
          These two values can be used to extract a snow density measurement:

          $$
            SWE = depth \times density
          $$

          where $density$ is measured as a percentage of water density.
          </p>

          <p>
            As you can see from the above figure, these density measurements are not great — snow density as a percentage of water should never go above one.
            However, measuring snow density with remote sensing, particularily that of wet snow, is a hard problem, and so SNOTEL sites can be the best data available.
          </p>
        </div>
      </div>


      <h1 id="lidar">LiDAR</h1>

      <div class="row">
        <div class="col-md-8 col-xs-12">
          <p>
            LiDAR sensors use laser energy to measure the distance between a sensor and a reflective target; the most common type uses the two-way travel time of a pulse of laser energy to cacluate a range.

            $$ range = c \frac {t} {2} $$

            where $c$ is the speed of light and $t$ is the two-way travel time of the laser pulse.
            When mounted to a mobile platform such as an aircraft, a LiDAR scanner can provide a large number of range measurements that cover a large spatial area.
          </p>

          <p>
            A basic LiDAR system is a one-dimensional measurement tool — it measures ranges from the scanner to the target.
            In order to measure points in two dimensions, scanners use a rotating or oscillating mirror to redirect the laser energy.
            A full three dimensions of measurement are enabled by moving the mirror in space, as in the case in mobile and airborne scanning, or rotating it around another axis, as is the case with terrestrial scanning.
          </p>

          <p>
            Even though we mount a LiDAR scanner on a moving platform, the scanner itself still only measures the range to a target.
            These range measurements must be combined with position and attitude information to get an x-y-z point in a global reference frame, in a process called <strong>georegistration</strong> or <strong>georeferencing</strong>.
            The LiDAR georeferencing equation is well-established<sup id="fnref:1"><a href="#fn:1">1</a></sup>:

            $$ \mathbf{X} = \mathbf{X}^g + \mathbf{R}^g \left( \mathbf{R}^l \mathbf{X}^s + \mathbf{X}^l \right) $$

            where $\mathbf{X}$ is the final laser point in the global reference frame, $\mathbf{X}^g$ is the position of the GNSS receiver in the global reference frame, $\mathbf{R}^g$ is the rotation matrix for the Interital Motion Unit (IMU) that rotates the point from local level to the global reference frame, $\mathbf{R}^l$ is the rotation matrix of the rotation displacement between the IMU and the LiDAR scanner that rotates the point from the scanner's own coordinate system to local level, $\mathbf{X}^s$ is the scanned point in the Scanner's Own Coordinate System (SOCS), and $\mathbf{X}^l$ is the positional displacement between the GNSS receiver and the LiDAR scanner's origin in the local level reference frame.
            Creating a georeferenced point cloud from mobile LiDAR data is then a matter of mounting a GNSS/IMU unit rigidly to the same body as the scanner and applying the georeferencing equation to each LiDAR point.<sup id="fnref:2"><a href="#fn:2">2</a></sup>
          </p>
        </div>

        <div class="col-md-4 col-md-offset-0 col-sm-8 col-sm-offset-2 col-xs-12">
          <div class="thumbnail">
            <div class="caption">
              <h4>LiDAR internals</h4>

              <p>
                The laser source transmits pulses of laser energy, which are reflected by the rotating polygonal mirror and bounce off of a target.
                The time between pulse emission and energy detection is measured, and that time difference is used to calculate the range to target.
              </p>
            </div>
            <img src="images/lidar-schematic.svg" alt="snow depth">
          </div>
        </div>
      </div>

      <h2>Full-waveform LiDAR</h2>

      <div class="row">
        <div class="col-xs-12">
          <p>
            The idea that LiDAR scanners measure points in space is a bit of a simplification.
            To see why, you need to know a bit about how the energy returned to a LiDAR scanner is actually measured.
          </p>

          <p>
            When laser energy bounces off of a target and returns to the scanner, it passes back through some optics until it hits one or more more detectors.<sup id="fnref:3"><a href="#fn:3">3</a></sup>
            These detectors are very similar to those that exist in your digital camera — they record the intensity of the incident energy on the detector.
            The detectors in a LiDAR system are hooked up to some very precise timing machinery that records the time of the data sample with about a nanosecond resolution.
            This produces a timeseries of samples of the incident energy on the scanner's detector.
          </p>

          <p>
            Some examples of real-world waveform data are shown below.
          </p>
        </div>
      </div>

      <div class="row">
        <div class="col-md-6 col-xs-12">
          <div class="thumbnail">
            <img src="images/reference-pulse.png" alt="reference pulse">
            <div class="caption">
              <h3>Reference pulse</h3>
              <p>The energy of the laser pulse as it leaves the scanner.</p>
            </div>
          </div>
        </div>
        <div class="col-md-6 col-xs-12">
          <div class="thumbnail">
            <img src="images/return-pulse.png" alt="reference pulse">
            <div class="caption">
              <h3>Return pulse</h3>
              <p>
                The energy of the laser pulse as it returns to the scanner.
                The scanner that collected these data has two channels, high and low, one which is more sensitive than the other.
                Notice the multiple peaks in the low channel, indicating that this laser pulse interacted with multiple targets.
            </div>
          </div>
        </div>
      </div>

      <div class="row">
        <div class="col-xs-12">
          <p>
          Turning these full waveform data samples into points is a bit of a tricky problem.
          You can use the local maximums of the return data, but that can have trouble in cluttered or otherwise non-ideal environments.
          Many people use a Gaussian decomposition to turn the returned waveform into the summation of several Gaussian pulses, but that is imperfect if the outgoing laser energy isn't Gaussian (which it usually isn't).
          </p>

          <p>
            For this work, we use a simple peak detection method to convert our full waveform data to discrete points.
            We would have loved to implement a full Gaussian decomposition, but we ran out of time.
          </p>
        </div>
      </div>

      <h2>Error propagation</h2>

      <div class="row">
        <div class="col-xs-12">
          <p>
            As you're seeing, there are a lot of components that go into collecting a single LiDAR measurement, and each one of these components has an associated uncertainty of measurement.
            Turning our full waveform data into points has uncertainty.
            The measured angle of the mirror when the laser pulse is emitted has uncertainty.
            The position and attitude of the plane has uncertainty.
            All of these factors contribute to some percentage of the total uncertainty in the final x-y-z coordinates of our LiDAR points.
          </p>

          <p>
            When we compute the uncertainty of a computed quantity, the usual procedure is to use the General Law for the Propagation of Variances.<sup id="fnref:4"><a href="#fn:4">4</a></sup>
            We compute a 3x3 covariance matrix $\mathbf{C}$ for each point using the following relationships:

            $$ 
            \mathbf{C} = 
              \begin{bmatrix}
                \sigma_x^2 & \sigma_{xy} & \sigma_{xz} \\
                \sigma_{xy} & \sigma_y^2 & \sigma_{yz} \\
                \sigma_{xz} & \sigma_{yz} & \sigma_z^2
              \end{bmatrix}
              = \mathbf{A} \mathbf{C}_l \mathbf{A}^T
            $$

            where $\mathbf{A}$ is the <em>Jacobian</em> matrix of the partial derivatives of the Taylor series expansion of Equation 3, truncated at the first term, and $\mathbf{C}_l$ is the covariance matrix of variances and covariances for each observed quanity.
              
          </p>
        </div>
      </div>

      <h2>Error sources</h2>

      <div class="row">
        <div class="col-xs-12 col-md-4">
          <p>
          There are fourteen measurements that go into the measurement of a point $\mathbf{X}$:
          </p>

          <ul>
            <li>The range $r$ and angle $\theta$ measured by the LiDAR system</li>
            <li>The x, y, and z displacement $\mathbf{X}^l$ between the origin of the laser system and the origin of the GNSS system (lever arm)</li>
            <li>The roll $d\omega$, pitch $d\phi$, and yaw $d\kappa$ rotational offsets between the scanner's coordiante system and the IMU's coordinate system (boresight)</li>
            <li>The roll $\omega$, pitch $\phi$, and yaw $\kappa$ measured by the IMU</li>
            <li>The x, y, and z positions $\mathbf{X}^g$, as measured by the GNSS</li>
          </ul>

          <p>
            We can use these fourteen unknowns to calculate the Jacobian matrix as required for Equation 3.
          </p>
        </div>

        <div class="col-xs-12 col-md-8">
          <h4>X, as a function of the fourteen unknowns</h4>
          <p>
            $$
              \mathbf{X}_0 = \\
                \left( \cos\omega \cos\kappa + \sin\phi\sin\omega\sin\kappa \right) \\
                \left[ \left( \cos d \omega \cos d \kappa + \sin d \phi \sin d \omega \sin d \kappa \right) r \cos \theta +
                  \left( \cos d \kappa \sin d \omega - \cos d \omega \sin d \phi \sin d \kappa \right) r \sin \theta + \mathbf{X}^l_0 \right] + \\
                \cos\phi\sin\kappa \\
                \left[\left(\cos d\kappa \sin d\phi \sin d\omega - \cos d\omega \sin d\kappa \right) r\cos\theta + 
                  \left(-\sin d\omega \sin d\kappa - \cos d\omega \cos d\kappa \sin d\phi \right) r\sin \theta + \mathbf{X}^l_1 \right] + \\
                \left(\cos\kappa\sin\omega - \cos\omega\sin\phi\sin\kappa\right)
                \left[\left(-\sin d\phi \sin d\omega\right) r \cos\theta + \left(\cos d\phi \cos d\omega\right) r \sin\theta + \mathbf{X}^l_2 \right] + \mathbf{X}^g_0
            $$
          </p>
          <p>
            Similar equations for Y and Z are elided.
          </p>
        </div>
      </div>

      <div class="row">
        <div class="col-xs-12">
          <p>
            Some of the errors, such as those in the position and attidue of the IMU/GNSS system, are products of the IMU/GNSS processing and are available as timestamped records.
            Other errors must be estimated or measured.
            Per Glennie, the lever-arm errors are commonly assumed to be about 2cm each, and the boresight errors can be assumed to be 0.001° in roll and pitch and 0.004° in yaw.
            Errors in the range of scanner measurements are generally a function of the timing circuitry in the scanner are are usually on the order of 1cm to 2cm.
            Angular errors in the scanner measurements are complicated due to the divergence of the laser beam — Glennie discussuses reasonable simplifications to get a single angular value.
          </p>

          <p>
          For this work, we do not take into acount the uncertainty due to the interaction of the laser energy with the terrain.
          For more discussion on that topic, see Hartzell.<sup id="fnref:9"><a href="#fn:9">4</a></sup>
          </p>
        </div>
      </div>

      <h1 id="work">Work completed and future work</h1>

      <div class="row">
        <div class="col-xs-12">
          <p>
          Unfortunately, I was not able to complete my objective of propogating errors through to final snow depth and snow water equivilient data products.
          I am using data from the Airborne Snow Observatory collected in the 2013 and 2014 snow years, and those data that were provided to me did not include an vital intermediate data product (*.sdc files, which are the discretized and georeferenced point cloud data).
          These data were also incomplete enough that I was unable to process them using Riegl's proprietary software.
          Therefore, while I waited on a as-of-yet unfulfilled request for additional data from ASO, I (perhaps unwisely) undertook to implement my own full-waveform peak detection and georeferencing software.
          </p>

          <p>
            I was able to make good headway on the problem.
            I implemented custom peak detection<sup id="fnref:6"><a href="#fn:6">7</a></sup> for sdf files<sup id="fnref:7"><a href="#fn:7">7</a></sup>, one of Riegl's full-waveform data formats.
            I implemented georeferencing<sup id="fnref:8"><a href="#fn:8">8</a></sup> and demonstrated its correctness using another airborne discrete-point dataset.
            However, the integration of my full-waveform ASO data and my georeferencing package is, as of this writing, not working.
          </p>

          <p>
            Future work includes, of course, to either correctly georeference the full-waveform ASO data, or to get processed discrete-return data from ASO, which can then be used to complete the project objectives.
          </p>
        </div>
      </div>

      <div class="footnotes">
        <ol>
        <li class="footnote" id="fn:1">
          Glennie, C. L. (2007). Rigorous 3D error analysis of kinematic scanning LIDAR systems. Journal of Applied Geodesy, 1(3).
          <a href="#fnref:1" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:2">
          Since LiDAR points are not collected at evenly spaced intervals in time, GNSS/IMU measurements must be interpolated.
          <a href="#fnref:2" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:3">
           The scanner that we used has two channels, one which is more sensitive than the other. 
           This is to provide a wider dynamic range for signal detection.
          <a href="#fnref:3" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:4">
          Ghilani, C. D. (2010). Adjustment Computations.
          <a href="#fnref:4" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:5">
          <a href="http://www.wcc.nrcs.usda.gov/snow/">http://www.wcc.nrcs.usda.gov/snow/</a>
          <a href="#fnref:5" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:6">
          <a href="https://github.com/gadomski/peakbag">https://github.com/gadomski/peakbag</a>
          <a href="#fnref:6" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:7">
          <a href="https://github.com/gadomski/sdf-rs">https://github.com/gadomski/sdf-rs</a>
          <a href="#fnref:7" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:8">
          <a href="https://github.com/gadomski/georef">https://github.com/gadomski/georef</a>
          <a href="#fnref:8" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:9">
          Hartzell, P. J., Gadomski, P. J., Glennie, C. L., Finnegan, C., & Deems, J. S. (2015). Rigorous Error Propagation for Terrestrial Laser Scanning with Application to Snow Volume Uncertainty.
          <a href="#fnref:9" title="return to content"> &#8617;</a>
        </li>
      </ol>
      </div>
    </div>

    <nav class="navbar navbar-default navbar-fixed-bottom" id="navbar-bottom">
      <div class="container-fluid">
        <ul class="nav navbar-nav">
          <li><a href="#top">Introduction</a></li>
          <li><a href="#why">Why?</a></li>
          <li><a href="#snow-depths-and-densities">Snow depths and densities</a></li>
          <li><a href="#lidar">LiDAR</a></li>
          <li><a href="#work">Work completed and future work</a></li>
        </ul>
      </div>
    </nav>

    <script src="bower_components/jquery/dist/jquery.min.js"></script>
    <script src="bower_components/bootstrap/dist/js/bootstrap.min.js"></script>
    <script src="bower_components/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: { inlineMath: [["$","$"]] },
  TeX: { equationNumbers: { autoNumber: "all" } }
});
    </script>
    <script src="bower_components/bigfoot/dist/bigfoot.min.js"></script>
    <script type="text/javascript">
$.bigfoot();
    </script>

  </body>
</html>
