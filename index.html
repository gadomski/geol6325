<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>GEOL 6325 final project</title>

    <link href="bower_components/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="bower_components/bigfoot/dist/bigfoot-default.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <style>
body { position: relative; margin-bottom: 60px; }
    </style>

  </head>
  <body data-spy="scroll" data-target="#navbar-bottom">
    <div class="container">
      <div class="jumbotron">
        <h3>Calculating LiDAR point cloud uncertainty and propagating uncertainty to snow-water equivalent data products</h3>
        <p>GEOL 6325 final project<br>Peter J. Gadomski</p>
      </div>

      <div class="row">
        <div class="col-xs-10 col-xs-offset-1">
          <p class="lead">
            We collected a pair of LiDAR surveys over a mountain range, one when the area was snow-free and the other when it was snow-covered
            We performed full-waveform decomposition on these files, and attempted (and failed) to georegister them.
            We demonstrate the procedure for calculating snow depths and SWE measurements with propagated errors.
          </p>
        </div>
      </div>

      <h1>LiDAR</h1>

      <div class="row">
        <div class="col-md-8 col-xs-12">
          <p>
            LiDAR sensors use laser energy to measure the distance between a sensor and a reflective target; the most common type uses the two-way travel time of a pulse of laser energy to cacluate a range.

            $$ range = c \frac {t} {2} $$

            where $c$ is the speed of light and $t$ is the two-way travel time of the laser pulse.
            When mounted to a mobile platform such as an aircraft, a LiDAR scanner can provide a large number of range measurements that cover a large spatial area.
          </p>

          <p>
            A basic LiDAR system is a one-dimensional measurement tool — it measures ranges from the scanner to the target.
            In order to measure points in two dimensions, scanners use a rotating or oscillating mirror to redirect the laser energy.
            A full three dimensions of measurement are enabled by moving the mirror in space, as in the case in mobile and airborne scanning, or rotating it around another axis, as is the case with terrestrial scanning.
          </p>

          <p>
            Even though we mount a LiDAR scanner on a moving platform, the scanner itself still only measures the range to a target.
            These range measurements must be combined with position and attitude information to get an x-y-z point in a global reference frame, in a process called <strong>georegistration</strong> or <strong>georeferencing</strong>.
            The LiDAR georeferencing equation is well-established<sup id="fnref:1"><a href="#fn:1">1</a></sup>:

            $$ \mathbf{X} = \mathbf{X}^g + \mathbf{R}^l \left( \mathbf{R}^l \mathbf{X}^s + \mathbf{X}^l \right) $$

            where $\mathbf{X}$ is the final laser point in the global reference frame, $\mathbf{X}^g$ is the position of the GNSS receiver in the global reference frame, $\mathbf{R}^l$ is the rotation matrix for the Interital Motion Unit (IMU) that rotates the point from local level to the global reference frame, $\mathbf{R}^l$ is the rotation matrix of the rotation displacement between the IMU and the LiDAR scanner that rotates the point from the scanner's own coordinate system to local level, $\mathbf{X}^s$ is the scanned point in the Scanner's Own Coordinate System (SOCS), and $\mathbf{X}^l$ is the positional displacement between the GNSS receiver and the LiDAR scanner's origin in the local level reference frame.
            Creating a georeferenced point cloud from mobile LiDAR data is then a matter of mounting a GNSS/IMU unit rigidly to the same body as the scanner and applying the georeferencing equation to each LiDAR point.<sup id="fnref:2"><a href="#fn:2">2</a></sup>
          </p>
        </div>

        <div class="col-md-4 col-md-offset-0 col-sm-8 col-sm-offset-2 col-xs-12">
          <div class="thumbnail">
            <div class="caption">
              <h4>LiDAR internals</h4>

              <p>
                The laser source transmits pulses of laser energy, which are reflected by the rotating polygonal mirror and bounce off of a target.
                The time between pulse emission and energy detection is measured, and that time difference is used to calculate the range to target.
              </p>
            </div>
            <img src="images/lidar-schematic.svg" alt="snow depth">
          </div>
        </div>
      </div>

      <h2>Full-waveform LiDAR</h2>

      <div class="row">
        <div class="col-xs-12">
          <p>
            The idea that LiDAR scanners measure points in space is a bit of a simplification.
            To see why, you need to know a bit about how the energy returned to a LiDAR scanner is actually measured.
          </p>

          <p>
            When laser energy bounces off of a target and returns to the scanner, it passes back through some optics until it hits one or more more detectors.<sup id="fnref:3"><a href="#fn:3">3</a></sup>
            These detectors are very similar to those that exist in your digital camera — they record the intensity of the incident energy on the detector.
            The detectors in a LiDAR system are hooked up to some very precise timing machinery that records the time of the data sample with about a nanosecond resolution.
            This produces a timeseries of samples of the incident energy on the scanner's detector.
          </p>

          <p>
            Some examples of real-world waveform data are shown below.
          </p>
        </div>
      </div>

      <div class="row">
        <div class="col-md-6 col-xs-12">
          <div class="thumbnail">
            <img src="images/reference-pulse.png" alt="reference pulse">
            <div class="caption">
              <h3>Reference pulse</h3>
              <p>The energy of the laser pulse as it leaves the scanner.</p>
            </div>
          </div>
        </div>
        <div class="col-md-6 col-xs-12">
          <div class="thumbnail">
            <img src="images/return-pulse.png" alt="reference pulse">
            <div class="caption">
              <h3>Return pulse</h3>
              <p>
                The energy of the laser pulse as it returns to the scanner.
                The scanner that collected these data has two channels, high and low, one which is more sensitive than the other.
                Notice the multiple peaks in the low channel, indicating that this laser pulse interacted with multiple targets.
            </div>
          </div>
        </div>
      </div>

      <div class="row">
        <div class="col-xs-12">
          <p>
          Turning these full waveform data samples into points is a bit of a tricky problem.
          You can use the local maximums of the return data, but that can have trouble in cluttered or otherwise non-ideal environments.
          Many people use a Gaussian decomposition to turn the returned waveform into the summation of several Gaussian pulses, but that is imperfect if the outgoing laser energy isn't Gaussian (which it usually isn't).
          </p>

          <p>
            For this work, we use a simple peak detection method to convert our full waveform data to discrete points.
            We would have loved to implement a full Gaussian decomposition, but we ran out of time.
          </p>
        </div>
      </div>

      <h2>Error propagation</h2>

      <div class="row">
        <div class="col-xs-12">
          <p>
            As you're seeing, there are a lot of components that go into collecting a single LiDAR measurement, and each one of these components has an associated uncertainty of measurement.
            Turning our full waveform data into points has uncertainty.
            The measured angle of the mirror when the laser pulse is emitted has uncertainty.
            The position and attitude of the plane has uncertainty.
            All of these factors contribute to some percentage of the total uncertainty in the final x-y-z coordinates of our LiDAR points.
          </p>

          <p>
            When we compute the uncertainty of a computed quantity, the usual procedure is to use the General Law for the Propagation of Variances.<sup id="fnref:4"><a href="#fn:4">4</a></sup>
            We compute a 3x3 covariance matrix $\mathbf{C}$ for each point using the following relationships:

            $$ 
            \mathbf{C} = 
              \begin{bmatrix}
                \sigma_x^2 & \sigma_{xy} & \sigma_{xz} \\
                \sigma_{xy} & \sigma_y^2 & \sigma_{yz} \\
                \sigma_{xz} & \sigma_{yz} & \sigma_z^2
              \end{bmatrix}
              = \mathbf{A} \mathbf{C}_l \mathbf{A}^T
            $$

            where $\mathbf{A}$ is the <em>Jacobian</em> matrix of the partial derivatives of the Taylor series expansion of Equation 3, truncated at the first term, and $\mathbf{C}_l$ is the covariance matrix of variances and covariances for each observed quanity.
              
          </p>
        </div>
      </div>

      <h2>Error sources</h2>

      <div class="row">
        <div class="col-xs-12 col-md-6">
          <p>
            There are fourteen measurements that go into Equation 2:
          </p>

          <ul>
            <li>The range and angle measured by the LiDAR system</li>
            <li>The x, y, and z displacement between the origin of the laser system and the origin of the GNSS system (lever arm)</li>
            <li>The roll, pitch, and yaw rotational offsets between the scanner's coordiante system and the IMU's coordinate system (boresight)</li>
            <li>The roll, pitch, and yaw measured by the IMU</li>
            <li>The x, y, and z positions, as measured by the GNSS</li>
          </ul>

          <p>
            Each measurement has an associated uncertainty.
          </p>
        </div>
      </div>

      <h1>Snow depths</h1>

      <div class="row">
        <div class="col-md-4 col-xs-12">
          <div class="thumbnail">
            <img src="images/snow-depth.svg" alt="snow depth">
            <div class="caption">
              <h4>Calculating snow depth from two LiDAR scans</h4>

              <p>
                Subtract the height of the snow-on scan from the point height of the snow-free scan.
                Because points are unordered, a snow-on point will probably not have a snow-free point directly beneath it; use an interpolation method to calculate a snow-free height for each snow-on point.
              </p>
            </div>
          </div>
        </div>
        <div class="col-md-8 col-xs-12">
          <p>
            We can calculate a snow depth by comparing two georeferenced LiDAR scans &mdash; one scan with snow and the other without.
            We used data from the Airborne Snow Observatory (ASO)<sup id="fnref:5"><a href="#fn:5">5</a></sup>
          </p>
        </div>
      </div>

      <div class="footnotes">
        <ol>
        <li class="footnote" id="fn:1">
          Glennie, C. L. (2007). Rigorous 3D error analysis of kinematic scanning LIDAR systems. Journal of Applied Geodesy, 1(3).
          <a href="#fnref:1" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:2">
          Since LiDAR points are not collected at evenly spaced intervals in time, GNSS/IMU measurements must be interpolated.
          <a href="#fnref:2" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:3">
           The scanner that we used has two channels, one which is more sensitive than the other. 
           This is to provide a wider dynamic range for signal detection.
          <a href="#fnref:3" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:4">
          Ghilani, C. D. (2010). Adjustment Computations.
          <a href="#fnref:4" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:5">
          <a href="http://aso.jpl.nasa.gov/">http://aso.jpl.nasa.gov/</a>
          <a href="#fnref:5" title="return to content"> &#8617;</a>
        </li>
      </ol>
      </div>
    </div>

    <nav class="navbar navbar-default navbar-fixed-bottom" id="navbar-bottom">
      <div class="container-fluid">
        <ul class="nav navbar-nav">
          <li><a href="#pulse-based-lidar">Pulse-based LiDAR</a></li>
          <li><a href="#georeferencing">Georeferencing</a></li>
          <li><a href="#point-uncertainty">Point uncertainty</a></li>
        </ul>
      </div>
    </nav>

    <script src="bower_components/jquery/dist/jquery.min.js"></script>
    <script src="bower_components/bootstrap/dist/js/bootstrap.min.js"></script>
    <script src="bower_components/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: { inlineMath: [["$","$"]] },
  TeX: { equationNumbers: { autoNumber: "all" } }
});
    </script>
    <script src="bower_components/bigfoot/dist/bigfoot.min.js"></script>
    <script type="text/javascript">
$.bigfoot();
    </script>

  </body>
</html>
