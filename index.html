<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>GEOL 6325 final project</title>

    <link href="bower_components/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="bower_components/bigfoot/dist/bigfoot-default.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <style>
body { position: relative; margin-bottom: 60px; }
    </style>

  </head>
  <body data-spy="scroll" data-target="#navbar-bottom">
    <div class="container">
      <div class="jumbotron">
        <h3>Calculating LiDAR point cloud uncertainty and propagating uncertainty to snow-water equivalent data products</h3>
        <p>GEOL 6325 final project<br>Peter J. Gadomski</p>
      </div>

      <div class="row">
        <div class="col-xs-10 col-xs-offset-1">
          <p class="lead">
            Using LiDAR sensors, we collect high resolution spatial data over large regions.
            We collected a pair of LiDAR surveys, one when the area was snow-free and the other when it was snow-covered, and calculated snow depths.
            We then coupled these snow depths with snow density measurements to produce snow water equivalent (SWE) measurements.
          </p>

          <p class="lead">
            Each measurement used to calculate the final SWE values contains error.
            We propagate these measurement errors through georegistration and post-processing, and provide SWE values with uncertainty estimates.
          </p>
        </div>
      </div>

      <h1>LiDAR</h1>

      <div class="row">
        <div class="col-md-8 col-xs-12">
          <p>
            LiDAR sensors use laser energy to measure the distance between a sensor and a reflective target; the most common type uses the two-way travel time of a pulse of laser energy to cacluate a range.

            $$ range = c \frac {t} {2} $$

            where $c$ is the speed of light and $t$ is the two-way travel time of the laser pulse.
            When mounted to a mobile platform such as an aircraft, a LiDAR scanner can provide a large number of range measurements that cover a large spatial area.
          </p>

          <p>
            A basic LiDAR system is a one-dimensional measurement tool — it measures ranges from the scanner to the target.
            In order to measure points in two dimensions, scanners use a rotating or oscillating mirror to redirect the laser energy.
            A full three dimensions of measurement are enabled by moving the mirror in space, as in the case in mobile and airborne scanning, or rotating it around another axis, as is the case with terrestrial scanning.
          </p>

          <p>
            Even though we mount a LiDAR scanner on a moving platform, the scanner itself still only measures the range to a target.
            These range measurements must be combined with position and attitude information to get an x-y-z point in a global reference frame, in a process called <strong>georegistration</strong> or <strong>georeferencing</strong>.
            The LiDAR georeferencing equation is well-established<sup id="fnref:1"><a href="#fn:1">1</a></sup>:

            $$ X_{g} = X_{gnss} + R_{imu} \left( R_{boresight} X_{scanner} + X_{lever arm} \right) $$

            where $X_{g}$ is the final point in a global reference frame, $X_{gnss}$ is the position of the GNSS receiver in the global reference frame, $R_{imu}$ is the rotation matrix for the Interital Motion Unit (IMU), $R_{boresight}$ is the rotation matrix of the rotation displacement between the IMU and the LiDAR scanner, $X_{scanner}$ is the scanned point in the Scanner's Own Coordinate System (SOCS), and $X_{lever arm}$ is the positional displacement between the GNSS receiver and the LiDAR scanner's origin.
            Creating a georeferenced point cloud from mobile LiDAR data is then a matter of mounting a GNSS/IMU unit rigidly to the same body as the scanner and applying the georeferencing equation to each LiDAR point.<sup id="fnref:2"><a href="#fn:2">2</a></sup>.
          </p>
        </div>

        <div class="col-md-4 col-md-offset-0 col-sm-8 col-sm-offset-2 col-xs-12">
          <div class="thumbnail">
            <div class="caption">
              <h4>LiDAR internals</h4>

              <p>
                The laser source transmits pulses of laser energy, which are reflected by the rotating polygonal mirror and bounce off of a target.
                The time between pulse emission and energy detection is measured, and that time difference is used to calculate the range to target.
              </p>
            </div>
            <img src="images/lidar-schematic.svg" alt="snow depth">
          </div>
        </div>
      </div>

      <h2>Full-waveform LiDAR</h2>

      <div class="row">
        <div class="col-xs-12">
          <p>
            The idea that LiDAR scanners measure points in space is a bit of a simplification.
            To see why, you need to know a bit about how the energy returned to a LiDAR scanner is actually measured.
          </p>

          <p>
            When laser energy bounces off of a target and returns to the scanner, it passes back through some optics until it hits one or more more detectors.<sup id="fnref:3"><a href="#fn:3">3</a></sup>
            These detectors are very similar to those that exist in your digital camera — they record the intensity of the incident energy on the detector.
            The detectors in a LiDAR system are hooked up to some very precise timing machinery that records the time of the data sample with about a nanosecond resolution.
            This produces a timeseries of samples of the incident energy on the scanner's detector.
          </p>

          <p>
            Some examples of real-world waveform data are shown below.
          </p>
        </div>
      </div>

      <div class="row">
        <div class="col-md-6 col-xs-12">
          <div class="thumbnail">
            <img src="images/reference-pulse.png" alt="reference pulse">
            <div class="caption">
              <h3>Reference pulse</h3>
              <p>The energy of the laser pulse as it leaves the scanner.</p>
            </div>
          </div>
        </div>
        <div class="col-md-6 col-xs-12">
          <div class="thumbnail">
            <img src="images/return-pulse.png" alt="reference pulse">
            <div class="caption">
              <h3>Return pulse</h3>
              <p>
                The energy of the laser pulse as it returns to the scanner.
                The scanner that collected these data has two channels, high and low, one which is more sensitive than the other.
                Notice the multiple peaks in the low channel, indicating that this laser pulse interacted with multiple targets.
            </div>
          </div>
        </div>
      </div>

      <div class="row">
        <div class="col-xs-12">
          <p>
          Turning these full waveform data samples into points is a bit of a tricky problem.
          You can use the local maximums of the return data, but that can have trouble in cluttered or otherwise non-ideal environments.
          Many people use a Gaussian decomposition to turn the returned waveform into the summation of several Gaussian pulses, but that is imperfect if the outgoing laser energy isn't Gaussian (which it usually isn't).
          </p>

          <p>
            For this work, we use a simple peak detection method to convert our full waveform data to discrete points.
            We would have loved to implement a full Gaussian decomposition, but we ran out of time.
          </p>
        </div>
      </div>

      <h2>Error propagation</h2>

      <div class="row">
        <div class="col-md-6 col-xs-12">
          <p>
            As you're seeing, there are a lot of components that go into collecting a single LiDAR measurement, and each one of these components has an associated uncertainty of measurement.
            Turning our full waveform data into points has uncertainty.
            The measured angle of the mirror when the laser pulse is emitted has uncertainty.
            The position and attitude of the plane has uncertainty.
            All of these factors contribute to some percentage of the total uncertainty in the final x-y-z coordinates of our LiDAR points.
          </p>

          <p>
            When we compute the uncertainty of a computed quantity, the usual procedure is to use the General Law for the Propagation of Variances.
            If $f$ is a function of $x_0$, $x_1$, ..., then the GLOPV can be described by Equation 3.
          </p>

          <p>
            If $f$ is non-linear, as our georeferencing equation is, then those partial derivatives will probably bit a bit tricky to compute.
            The common practice is to linearize the equation by taking the Taylor series expansion and truncating it at the first term.
            This allows us to compute the incremental changes in our final position due to incremental changes in each of our measured components.
          </p>

          <p>
            For our work, we linearized the georeferencing equation (Equation 2) and propagated each component error to our final computed quantities.
          </p>
        </div>

        <div class="col-md-6 col-xs-12">
          <h4>The General Law for the Propagation of Variance</h4>
          <p>
          $$ f\left(x_0, x_1, ...\right) \\ \sigma_f^2 = \left( \frac {\delta f} {\delta x_0} \sigma_{x_0} \right)^2 + \left( \frac {\delta f} {\delta x_1} \sigma_{x_1} \right)^2 + ... $$
          </p>
        </div>
      </div>

      <h1>Snow depths</h1>

      <div class="row">
        <div class="col-md-4 col-xs-12">
          <div class="thumbnail">
            <img src="images/snow-depth.svg" alt="snow depth">
            <div class="caption">
              <h4>Calculating snow depth from two LiDAR scans</h4>

              <p>
                Subtract the height of the snow-on scan from the point height of the snow-free scan.
                Because points are unordered, a snow-on point will probably not have a snow-free point directly beneath it; use an interpolation method to calculate a snow-free height for each snow-on point.
              </p>
            </div>
          </div>
        </div>
        <div class="col-md-8 col-xs-12">
          <p>
            We can calculate a snow depth by comparing two georeferenced LiDAR scans &mdash; one scan with snow and the other without.
            We used data from the Airborne Snow Observatory (ASO)<sup id="fnref:4"><a href="#fn:4">4</a></sup>
          </p>
        </div>
      </div>

      <div class="footnotes">
        <ol>
        <li class="footnote" id="fn:1">
          Glennie, C. L. (2007). Rigorous 3D error analysis of kinematic scanning LIDAR systems. Journal of Applied Geodesy, 1(3).
          <a href="#fnref:1" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:2">
          Since LiDAR points are not collected at evenly spaced intervals in time, GNSS/IMU measurements must be interpolated.
          <a href="#fnref:2" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:3">
           The scanner that we used has two channels, one which is more sensitive than the other. 
           This is to provide a wider dynamic range for signal detection.
          <a href="#fnref:3" title="return to content"> &#8617;</a>
        </li>
        <li class="footnote" id="fn:4">
          <a href="http://aso.jpl.nasa.gov/">http://aso.jpl.nasa.gov/</a>
          <a href="#fnref:4" title="return to content"> &#8617;</a>
        </li>
      </ol>
      </div>
    </div>

    <nav class="navbar navbar-default navbar-fixed-bottom" id="navbar-bottom">
      <div class="container-fluid">
        <ul class="nav navbar-nav">
          <li><a href="#pulse-based-lidar">Pulse-based LiDAR</a></li>
          <li><a href="#georeferencing">Georeferencing</a></li>
          <li><a href="#point-uncertainty">Point uncertainty</a></li>
        </ul>
      </div>
    </nav>

    <script src="bower_components/jquery/dist/jquery.min.js"></script>
    <script src="bower_components/bootstrap/dist/js/bootstrap.min.js"></script>
    <script src="bower_components/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: { inlineMath: [["$","$"]] },
  TeX: { equationNumbers: { autoNumber: "all" } }
});
    </script>
    <script src="bower_components/bigfoot/dist/bigfoot.min.js"></script>
    <script type="text/javascript">
$.bigfoot();
    </script>

  </body>
</html>
